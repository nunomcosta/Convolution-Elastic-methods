

```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(IRon)
library(earth)
library(MASS)
library(rpart)
library(ranger)
library(digest)
library(e1071)
library(doParallel)
library(scam)
library(xgboost)
library(Metrics)
```









```{r}
#loading sample dataset
df <- (read.table(url("http://lib.stat.cmu.edu/datasets/PM10.dat"),sep = "\t", header=FALSE))
df$V1<-exp(df$V1)
df<-df[c(1:100),]
colnames(df)<-c("target","cars","temp","wind","temp_d","wind_d","time","idk")

#dataframe with final metrics plus distribution peak
model.performance<-data.frame(instance="dummy",rmse=0,sera=0,peak=0,  stringsAsFactors = FALSE)

#test-train split
X_train=df[c(1:round(nrow(df)*0.7)),]

X_test=df[c(round(nrow(df)*0.7+1):nrow(df)),]

#selecting iteration granularity
resol=5

#iteration over 2 standard deviations in train data
for (i in seq(-sd(X_train$target),sd(X_train$target),length.out=resol)) {

#creating phis and changing relevance profile
ph <- phi.control(X_train$target)


relevance<-c(ph[["control.pts"]][1]+i, ph[["control.pts"]][2], ph[["control.pts"]][3],
               ph[["control.pts"]][4], ph[["control.pts"]][5], ph[["control.pts"]][6],
                  ph[["control.pts"]][7]-i, ph[["control.pts"]][8], ph[["control.pts"]][9])

m1 <- matrix(relevance, ncol=3, byrow=TRUE)


ph <- phi.control(X_train$target, method="range",
                  control.pts=m1)

phi.trues <- IRon::phi(X_test$target,ph)



#one simple benchmark is the mean of test target
model.performance<-rbind(model.performance,c("mean",
                    sqrt(mse(X_test$target,mean(df$target))),
                    sera(X_test$target,mean(df$target),phi.trues),i) )


#this chunk is for training to optimize in function of SERA instead of normal regression metrics
#declaring sera function for traincontrol and optimization
SERA <- function (data,
                        lev = NULL,
                        model = NULL,
                        m=matrix(
                          c(ph[["control.pts"]][1]+i, ph[["control.pts"]][2], ph[["control.pts"]][3],
                            ph[["control.pts"]][4], ph[["control.pts"]][5], ph[["control.pts"]][6],
                            ph[["control.pts"]][7]-i, ph[["control.pts"]][8], ph[["control.pts"]][9]),ncol=3,byrow=TRUE)) 
  {
                                            
   out <- IRon::sera(data$obs, data$pred, IRon::phi(data$obs,      IRon::phi.control(data$obs, method="range",control.pts=m)))    
   names(out) <- "SERA"
   out
}


#Training models - using 3 algorithms -> mars, XGB and random forest

#MARS
# create a tuning grid
set.seed(123)
tune_control <- trainControl(
  summaryFunction = SERA,
  allowParallel = TRUE,
  method = "cv"
)


set.seed(123)
hyper_grid_mars <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 8, length.out = 8) %>% floor()
)



# cross validated model
tuned_mars <- train(
  x = subset(X_train, select = -target),
  y = X_train$target,
  method = "earth",
  metric = "SERA",
  trControl = tune_control,
  tuneGrid = hyper_grid_mars,
  maximize=FALSE
)




mars <- earth(
  target ~ .,  
  data = X_train,
  degree = tuned_mars[["bestTune"]][["degree"]],
  nprune = tuned_mars[["bestTune"]][["nprune"]]
)

pred<-predict(mars,X_test)

model.performance<-rbind(model.performance,c("mars",
                    rmse=sqrt(mse(X_test$target,pred)),
                    sera=sera(X_test$target,pred,phi.trues),i)
)










#XGB

set.seed(123)
len <- 15
hyper_grid_xgb <-
	data.frame(
		nrounds = sample(1:1000, size = len, replace = TRUE),
		max_depth = sample(1:10, replace = TRUE, size = len),
		eta = runif(len, min = .001, max = .6),
		gamma = runif(len, min = 0, max = 10),
		colsample_bytree = runif(len, min = .3, max = .7),
		min_child_weight = sample(0:20, size = len, replace = TRUE),
		subsample = runif(len, min = .25, max = 1)
	)




set.seed(123)
mod <- train(
	target ~ .,
	data = X_train,
	method = "xgbTree",
	trControl = tune_control,
	tuneGrid = hyper_grid_xgb,
	metric = "SERA",
	maximize=FALSE
)





xgb<-xgboost(
                 data=as.matrix(subset(X_train, select=-c(target))),
                 label=X_train$target,
                 objective = "reg:squarederror",
                 eta=mod[["bestTune"]][["eta"]],
                 max_depth=mod[["bestTune"]][["max_depth"]],
                 gamma=mod[["bestTune"]][["gamma"]],
                 colsample_bytree=mod[["bestTune"]][["colsample_bytree"]],
                 min_child_weight=mod[["bestTune"]][["min_child_weight"]],
                 subsample=mod[["bestTune"]][["subsample"]],
                 nrounds=mod[["bestTune"]][["nrounds"]]
                 )



pred<-predict(xgb, as.matrix(subset(X_test, select=-c(target))))

model.performance<-rbind(model.performance,c("XGB",
                    rmse=sqrt(mse(X_test$target,as.vector(pred))) ,
                    sera=sera(X_test$target,as.vector(pred),phi.trues),i)
                     )



#random forest


hyper_grid_rf <- expand.grid(
                           mtry      = seq(0, ncol(df)-2, length.out = 10),
                           splitrule = c("variance","extratrees"),
                           min.node.size    = seq(1, 5, by = 2)
                          )
#sampling
hyper_grid_rf <- hyper_grid_rf[sample(nrow(hyper_grid_rf),15),]




mod <- train(target ~ ., 
                 trControl=tune_control, 
                 method = "ranger", 
                 data=X_train,
                 tuneGrid = hyper_grid_rf,
                 importance = 'impurity',
                 num.trees=200,
                 metric="SERA",
                 maximize=FALSE
                 )


rf<-ranger(target ~ .,
               data=X_train,
               mtry=mod[["bestTune"]][["mtry"]],
               splitrule=mod[["bestTune"]][["splitrule"]],
               min.node.size=mod[["bestTune"]][["min.node.size"]],
               num.trees=200)



pred<-predict(rf, X_test)

model.performance<-rbind(model.performance,c("random forest",
                    rmse=sqrt(mse(X_test$target,as.vector(pred$predictions))),
                    sera=sera(X_test$target,as.vector(pred$predictions),phi.trues),i)
                    )











}


#plotting results

model.performance<-model.performance[-1,]
model.performance$sera<-as.double(model.performance$sera)
model.performance$peak<-as.double(model.performance$peak)

auxdf<-model.performance %>% filter(instance!="mean")

p<-ggplot(model.performance,aes(x=peak,y=sera,colour=instance,fill=instance,linetype=instance))+
  geom_point(alpha=.66)
p<-p+geom_smooth(method = lm, formula = y ~ splines::bs(x, 4), se = FALSE) + 
  scale_color_manual(values=c("#ED7D31",  "#000000","#70AD47", "#4472C4"))+
  scale_fill_manual(values=c("#ED7D31"  ,"#000000","#70AD47", "#4472C4"))+
  theme_minimal()+
    ylim(0,max(auxdf$sera)*2)+
  scale_linetype_manual(values =  c("solid", "dashed", "dotted","solid","solid"))+

  xlab("Deviation")+

   scale_x_continuous()+
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.line = element_line(),
        axis.ticks = element_line(),
        axis.text.x = element_text(angle = 45),
        plot.title=element_text(face="bold"),
        text = element_text(size = 17))
print(p)







```




